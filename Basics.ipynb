{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55639dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418a7af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = '''In the field of natural language processing, there are a variety of tasks such as automatic text classification, sentiment analysis, text summarization, etc. These tasks are partially based on the pattern of the sentence and the meaning of the words in a different context. The two different words may be similar with an amount of amplitude. For example, the words ‘jog’ and ‘run’, both of them are partially different and also partially similar to each other. To perform specific NLP-based tasks, it is required to understand the intuition of words in different positions and hold the similarity between the words as well. Here WordNET comes to the picture which helps in solving the linguistic problems of the NLP models.\n",
    "WordNET is a lexical database of semantic relations between words in more than 200 languages. In this article, we will discuss WordNet in detail with its structure, working and implementation.  The major points to be discussed in this article are listed below.WordNET is a lexical database of words in more than 200 languages in which we have adjectives, adverbs, nouns, and verbs grouped differently into a set of cognitive synonyms, where each word in the database is expressing its distinct concept. The cognitive synonyms which are called synsets are presented in the database with lexical and semantic relations. WordNET is publicly available for download and also we can test its network of related words and concepts using this link. Below are a few test images when accessed this through the browser.Where thesaurus is helping us in finding the synonyms and antonyms of the words the WordNET is helping us to do more than that. WordNET interlinks the specific sense of the words wherein thesaurus links words by their meaning only. In the WordNET the words are semantically disambiguated if they are in close proximity to each other. Thesaurus provides a level to the words in the network if the words have similar meaning but in the case of WordNET, we get levels of words according to their semantic relations which is a better way of grouping the words.The below image is a basic structure of the WordNET. The main concept of the relationship between the words in the WordNETs network is that the words are synonyms like sad and unhappy, benefit and profit. These words show the same concept of using them in similar contexts by interchanging them. These types of words are grouped into synsets which are unordered sets. Where synsets are linked together if they are having even small conceptual relations. Every synset in the network has its own brief definition and many of them are illustrated with the example of how to use them in a sentence. That definition and example part makes WordNET different from other. In the below picture we can see the structure of any synset where we are having synonyms of benefit in the array of synsets with the definition and the example of usage of benefit word. This synset is related to another synset word, where the words benefit and profit have exactly the same meaning. Here we can see the structure of the wordnet and also how the synsets under the networks are interlinked because of the conceptual relation between the words. In linguistics, a word with a broad meaning constitutes a category into which words with more specific meanings fall; a superordinate. For example, the colour is a hypernym of red. Where Hyponymy shows the relationship between a hypernym and a specific instance of a hyponym. A hyponym is a word or phrase whose semantic field is more specific than its hypernym. The semantic field of a hypernym, also known as a superordinate. The reason for explaining these terms here is because in WordNET the most frequent relationships between synsets are based on these hyponym and hypernym relations. These are very beneficial in linking words like(paper, piece of paper). Saying more specifically with an example from the above picture like purple and violet, in WordNET the category colour includes purple which in turn includes violet. The root node of the hierarchy is the last point for every noun. In violet is a kind of purple and purple is a kind of colour then violet is a kind colour this is the hyponymy relation between the words which is transitive. The wordnet hold follows the meronymy relation which defines the whole relationship between the synset for example a bike has two wheels handle and petrol tank. These components of a bike are inherited from their subordinates: if a bike has two wheels then a sports bike has wheels as well. In linguistics, we basically use this kind of relationship for adverbs which basically represents the characteristic of the noun. So the parts are inherited into a downward direction because all the bikes and types of bikes have two wheels, but not all kinds of automobiles consist of two wheels. Adjective words under the WordNET arranged in the antonymy pairs like wet and dry, smile and cry. Each of these pairs of antonyms is linked with sets of semantic similar ones. The cry is linked to weep, shed tears, sob, wail etc. so that they all can be considered as the opposite of indirect antonyms of a smile. Most of the relations in the wordNET are in the same part of speech. On the basis of part of speech relations, we can divide WordNET into 4 types of 4 subnets one for each noun, verbs, adjective, and adverb. There are also some cross-PoS pointers available in the network which include a morphosemantic link that holds the words with the same meaning and shares a stem. For example, many pairs like (reader read) in which the noun of the pair has a semantic layer with respect to the verb have been specified. Here in this article, we had an overview of the WordNET along with an understanding of what are the basic structures of the wordnet and the synset. We discussed how it works to make the relation between the words properly because the manageable representation of the data into the model can make a model more accurate and workable. We saw what lexical relation that the database follows to hold the word with huge information and we have seen how we can implement this using python and nltk. It can be done using TextBlob and R as well. You can use WordNET and try it with these tools also. And try to accurately implement it in the models for better accuracy.'''\n",
    "len(sentence2.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf05567",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428bad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (word_tokenize, sent_tokenize, TreebankWordTokenizer,\n",
    "                           TweetTokenizer, MWETokenizer, wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce24c23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White space: ['Hope,', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear!', '#Hope', '#Amal.M']\n",
      "\n",
      "Word: ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal.M']\n",
      "\n",
      "Sentence: ['Hope, is the only thing stronger than fear!', '#Hope #Amal.M']\n",
      "\n",
      "Punctuation: ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal', '.', 'M']\n",
      "\n",
      "MWETokenizer: ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger_than', 'fear', '!', '#', 'Hope', '#', 'Amal.M']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# White Space tokenization\n",
    "\n",
    "text = 'Hope, is the only thing stronger than fear! #Hope #Amal.M'\n",
    "print(f'White space: {text.split()}\\n')\n",
    "\n",
    "# Word and Sentence tokenizer\n",
    "print(f'Word: {word_tokenize(text)}\\n')\n",
    "print(f'Sentence: {sent_tokenize(text)}\\n')\n",
    "\n",
    "# Punctuation-based tokenizer\n",
    "print(f'Punctuation: {wordpunct_tokenize(text)}\\n')\n",
    "\n",
    "# Tweet Tokenizer: It accomodates emojies too\n",
    "\n",
    "# MWET:  It can merge multi-word expressions into single tokens.\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('stronger', 'than'))\n",
    "print(f'MWETokenizer: {tokenizer.tokenize(word_tokenize(text))}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3f530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.009971380233764648 for textblob seconds ---\n",
      "--- 0.0 for textblob seconds ---\n",
      "--- 0.6452956199645996 for spacy seconds ---\n",
      "--- 0.0 for gensim seconds ---\n",
      "Gensim: ['Hope', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', 'Hope', 'Amal', 'M']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nltk\n",
    "start_time = time.time()\n",
    "word_tokenize(sentence2)\n",
    "print(\"--- %s for textblob seconds ---\" % (time.time() - start_time))\n",
    "# --- 0.012963056564331055 for nltk seconds ---\n",
    "\n",
    "# TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "start_time = time.time()\n",
    "blob_obj = TextBlob(sentence2)\n",
    "print(\"--- %s for textblob seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# print(f'TextBlob: {blob_obj.words}\\n')\n",
    "# --- 0.000997304916381836 for textblob seconds ---\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "start_time = time.time()\n",
    "doc = nlp(sentence2)\n",
    "print(\"--- %s for spacy seconds ---\" % (time.time() - start_time))\n",
    "# --- 0.11168909072875977 for spacy seconds ---\n",
    "\n",
    "# Gensim\n",
    "from gensim.utils import tokenize\n",
    "start_time = time.time()\n",
    "t = tokenize(sentence2)\n",
    "print(\"--- %s for gensim seconds ---\" % (time.time() - start_time))\n",
    "print(f'Gensim: {list(tokenize(text))}\\n')\n",
    "# --- 0.000997304916381836 for gensim seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe96a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subword Tokenization:\n",
    "# Subword Tokenization splits the piece of text into subwords (or n-gram characters). For example, words like lower can be \n",
    "# segmented as low-er, smartest as smart-est, and so on.\n",
    "\n",
    "# Byte Pair Encoding (BPE):\n",
    "# Byte Pair Encoding BPE is a word segmentation algorithm that merges the most frequently occurring character or character \n",
    "# sequences iteratively. BPE tackles OOV(Out of Vocabulary: refer to the new words which are encountered at testing. These new \n",
    "# words do not exist in the vocabulary.) effectively. It segments OOV as subwords and represents the word in terms of these subwords.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503210f",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6e4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n",
      "friend              friend              friend              friend                        friend                                  \n",
      "friendship          friendship          friendship          friend                        friendship                              \n",
      "friends             friend              friend              friend                        friend                                  \n",
      "friendships         friendship          friendship          friend                        friendship                              \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "# Poster Stemmer\n",
    "# Porter Stemmer is the original stemmer and is renowned for its ease of use and rapidity. Frequently, the resultant stem is a \n",
    "# shorter word with the same root meaning.\n",
    "    \n",
    "# Snowball Stemmer \n",
    "# It is somewhat faster and more logical than the original Porter Stemmer.\n",
    "    \n",
    "# Lancaster Stemmer:\n",
    "# Lancaster Stemmer is straightforward, although it often produces results with excessive stemming.\n",
    "\n",
    "# Regexp Stemmer – RegexpStemmer()\n",
    "# Regex stemmer identifies morphological affixes using regular expressions. Substrings matching the regular expressions will be \n",
    "# discarded.\n",
    "    \n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5027e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088\n",
      "--- 0.0010039806365966797 for Regexp seconds ---\n",
      "--- 0.0 for Porter seconds ---\n",
      "--- 0.0029904842376708984 for Snowball seconds ---\n",
      "--- 0.0 for Lancaster seconds ---\n"
     ]
    }
   ],
   "source": [
    "sentence2 = '''In the field of natural language processing, there are a variety of tasks such as automatic text classification, sentiment analysis, text summarization, etc. These tasks are partially based on the pattern of the sentence and the meaning of the words in a different context. The two different words may be similar with an amount of amplitude. For example, the words ‘jog’ and ‘run’, both of them are partially different and also partially similar to each other. To perform specific NLP-based tasks, it is required to understand the intuition of words in different positions and hold the similarity between the words as well. Here WordNET comes to the picture which helps in solving the linguistic problems of the NLP models.\n",
    "WordNET is a lexical database of semantic relations between words in more than 200 languages. In this article, we will discuss WordNet in detail with its structure, working and implementation.  The major points to be discussed in this article are listed below.WordNET is a lexical database of words in more than 200 languages in which we have adjectives, adverbs, nouns, and verbs grouped differently into a set of cognitive synonyms, where each word in the database is expressing its distinct concept. The cognitive synonyms which are called synsets are presented in the database with lexical and semantic relations. WordNET is publicly available for download and also we can test its network of related words and concepts using this link. Below are a few test images when accessed this through the browser.Where thesaurus is helping us in finding the synonyms and antonyms of the words the WordNET is helping us to do more than that. WordNET interlinks the specific sense of the words wherein thesaurus links words by their meaning only. In the WordNET the words are semantically disambiguated if they are in close proximity to each other. Thesaurus provides a level to the words in the network if the words have similar meaning but in the case of WordNET, we get levels of words according to their semantic relations which is a better way of grouping the words.The below image is a basic structure of the WordNET. The main concept of the relationship between the words in the WordNETs network is that the words are synonyms like sad and unhappy, benefit and profit. These words show the same concept of using them in similar contexts by interchanging them. These types of words are grouped into synsets which are unordered sets. Where synsets are linked together if they are having even small conceptual relations. Every synset in the network has its own brief definition and many of them are illustrated with the example of how to use them in a sentence. That definition and example part makes WordNET different from other. In the below picture we can see the structure of any synset where we are having synonyms of benefit in the array of synsets with the definition and the example of usage of benefit word. This synset is related to another synset word, where the words benefit and profit have exactly the same meaning. Here we can see the structure of the wordnet and also how the synsets under the networks are interlinked because of the conceptual relation between the words. In linguistics, a word with a broad meaning constitutes a category into which words with more specific meanings fall; a superordinate. For example, the colour is a hypernym of red. Where Hyponymy shows the relationship between a hypernym and a specific instance of a hyponym. A hyponym is a word or phrase whose semantic field is more specific than its hypernym. The semantic field of a hypernym, also known as a superordinate. The reason for explaining these terms here is because in WordNET the most frequent relationships between synsets are based on these hyponym and hypernym relations. These are very beneficial in linking words like(paper, piece of paper). Saying more specifically with an example from the above picture like purple and violet, in WordNET the category colour includes purple which in turn includes violet. The root node of the hierarchy is the last point for every noun. In violet is a kind of purple and purple is a kind of colour then violet is a kind colour this is the hyponymy relation between the words which is transitive. The wordnet hold follows the meronymy relation which defines the whole relationship between the synset for example a bike has two wheels handle and petrol tank. These components of a bike are inherited from their subordinates: if a bike has two wheels then a sports bike has wheels as well. In linguistics, we basically use this kind of relationship for adverbs which basically represents the characteristic of the noun. So the parts are inherited into a downward direction because all the bikes and types of bikes have two wheels, but not all kinds of automobiles consist of two wheels. Adjective words under the WordNET arranged in the antonymy pairs like wet and dry, smile and cry. Each of these pairs of antonyms is linked with sets of semantic similar ones. The cry is linked to weep, shed tears, sob, wail etc. so that they all can be considered as the opposite of indirect antonyms of a smile. Most of the relations in the wordNET are in the same part of speech. On the basis of part of speech relations, we can divide WordNET into 4 types of 4 subnets one for each noun, verbs, adjective, and adverb. There are also some cross-PoS pointers available in the network which include a morphosemantic link that holds the words with the same meaning and shares a stem. For example, many pairs like (reader read) in which the noun of the pair has a semantic layer with respect to the verb have been specified. Here in this article, we had an overview of the WordNET along with an understanding of what are the basic structures of the wordnet and the synset. We discussed how it works to make the relation between the words properly because the manageable representation of the data into the model can make a model more accurate and workable. We saw what lexical relation that the database follows to hold the word with huge information and we have seen how we can implement this using python and nltk. It can be done using TextBlob and R as well. You can use WordNET and try it with these tools also. And try to accurately implement it in the models for better accuracy.'''\n",
    "print(len(sentence2.split()))\n",
    "\n",
    "start_time_reg = time.time()\n",
    "regexp.stem(sentence2)\n",
    "print(\"--- %s for Regexp seconds ---\" % (time.time() - start_time_reg))\n",
    "\n",
    "start_time_por = time.time()\n",
    "porter.stem(sentence2)\n",
    "print(\"--- %s for Porter seconds ---\" % (time.time() - start_time_por))\n",
    "\n",
    "start_time_snow = time.time()\n",
    "snowball.stem(sentence2)\n",
    "print(\"--- %s for Snowball seconds ---\" % (time.time() - start_time_snow))\n",
    "\n",
    "start_time_lan = time.time()\n",
    "lancaster.stem(sentence2)\n",
    "print(\"--- %s for Lancaster seconds ---\" % (time.time() - start_time_lan))\n",
    "\n",
    "# --- 0.0009987354278564453 for Regexp seconds ---\n",
    "# --- 0.0009970664978027344 for Porter seconds ---\n",
    "# --- 0.0029921531677246094 for Snowball seconds ---\n",
    "# --- 0.0010068416595458984 for Lancaster seconds ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdcfae",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c6d920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9f66afba18b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "del lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "738f42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.0178451538085938 seconds ---\n",
      "In the field of natural language processing , there are a variety of task such a automatic text classification , sentiment analysis , text summarization , etc . These task are partially based on the pattern of the sentence and the meaning of the word in a different context . The two different word may be similar with an amount of amplitude . For example , the word ‘ jog ’ and ‘ run ’ , both of them are partially different and also partially similar to each other . To perform specific NLP-based task , it is required to understand the intuition of word in different position and hold the similarity between the word a well . Here WordNET come to the picture which help in solving the linguistic problem of the NLP model . WordNET is a lexical database of semantic relation between word in more than 200 language . In this article , we will discus WordNet in detail with it structure , working and implementation . The major point to be discussed in this article are listed below.WordNET is a lexical database of word in more than 200 language in which we have adjective , adverb , noun , and verb grouped differently into a set of cognitive synonym , where each word in the database is expressing it distinct concept . The cognitive synonym which are called synset are presented in the database with lexical and semantic relation . WordNET is publicly available for download and also we can test it network of related word and concept using this link . Below are a few test image when accessed this through the browser.Where thesaurus is helping u in finding the synonym and antonym of the word the WordNET is helping u to do more than that . WordNET interlinks the specific sense of the word wherein thesaurus link word by their meaning only . In the WordNET the word are semantically disambiguated if they are in close proximity to each other . Thesaurus provides a level to the word in the network if the word have similar meaning but in the case of WordNET , we get level of word according to their semantic relation which is a better way of grouping the words.The below image is a basic structure of the WordNET . The main concept of the relationship between the word in the WordNETs network is that the word are synonym like sad and unhappy , benefit and profit . These word show the same concept of using them in similar context by interchanging them . These type of word are grouped into synset which are unordered set . Where synset are linked together if they are having even small conceptual relation . Every synset in the network ha it own brief definition and many of them are illustrated with the example of how to use them in a sentence . That definition and example part make WordNET different from other . In the below picture we can see the structure of any synset where we are having synonym of benefit in the array of synset with the definition and the example of usage of benefit word . This synset is related to another synset word , where the word benefit and profit have exactly the same meaning . Here we can see the structure of the wordnet and also how the synset under the network are interlinked because of the conceptual relation between the word . In linguistics , a word with a broad meaning constitutes a category into which word with more specific meaning fall ; a superordinate . For example , the colour is a hypernym of red . Where Hyponymy show the relationship between a hypernym and a specific instance of a hyponym . A hyponym is a word or phrase whose semantic field is more specific than it hypernym . The semantic field of a hypernym , also known a a superordinate . The reason for explaining these term here is because in WordNET the most frequent relationship between synset are based on these hyponym and hypernym relation . These are very beneficial in linking word like ( paper , piece of paper ) . Saying more specifically with an example from the above picture like purple and violet , in WordNET the category colour includes purple which in turn includes violet . The root node of the hierarchy is the last point for every noun . In violet is a kind of purple and purple is a kind of colour then violet is a kind colour this is the hyponymy relation between the word which is transitive . The wordnet hold follows the meronymy relation which defines the whole relationship between the synset for example a bike ha two wheel handle and petrol tank . These component of a bike are inherited from their subordinate : if a bike ha two wheel then a sport bike ha wheel a well . In linguistics , we basically use this kind of relationship for adverb which basically represents the characteristic of the noun . So the part are inherited into a downward direction because all the bike and type of bike have two wheel , but not all kind of automobile consist of two wheel . Adjective word under the WordNET arranged in the antonymy pair like wet and dry , smile and cry . Each of these pair of antonym is linked with set of semantic similar one . The cry is linked to weep , shed tear , sob , wail etc . so that they all can be considered a the opposite of indirect antonym of a smile . Most of the relation in the wordNET are in the same part of speech . On the basis of part of speech relation , we can divide WordNET into 4 type of 4 subnets one for each noun , verb , adjective , and adverb . There are also some cross-PoS pointer available in the network which include a morphosemantic link that hold the word with the same meaning and share a stem . For example , many pair like ( reader read ) in which the noun of the pair ha a semantic layer with respect to the verb have been specified . Here in this article , we had an overview of the WordNET along with an understanding of what are the basic structure of the wordnet and the synset . We discussed how it work to make the relation between the word properly because the manageable representation of the data into the model can make a model more accurate and workable . We saw what lexical relation that the database follows to hold the word with huge information and we have seen how we can implement this using python and nltk . It can be done using TextBlob and R a well . You can use WordNET and try it with these tool also . And try to accurately implement it in the model for better accuracy .\n"
     ]
    }
   ],
   "source": [
    "# Wordnet Lemmatizer with NLTK:\n",
    "# Wordnet is an large, freely and publicly available lexical database for the English language aiming to establish structured \n",
    "# semantic relationships between words.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "word_list = nltk.word_tokenize(sentence2)\n",
    "# print(word_list)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "start_time = time.time()\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(lemmatized_output)\n",
    "# --- 0.009977340698242188 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb021a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.000997781753540039 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "start_time = time.time()\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# --- 0.0009982585906982422 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf76a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the field of natural language processing there are a variety of task such a automatic text classification sentiment analysis text summarization etc These task are partially based on the pattern of the sentence and the meaning of the word in a different context The two different word may be similar with an amount of amplitude For example the word ‘ jog ’ and ‘ run ’ both of them are partially different and also partially similar to each other To perform specific NLP-based task it is required to understand the intuition of word in different position and hold the similarity between the word a well Here WordNET come to the picture which help in solving the linguistic problem of the NLP model WordNET is a lexical database of semantic relation between word in more than 200 language In this article we will discus WordNet in detail with it structure working and implementation The major point to be discussed in this article are listed below.WordNET is a lexical database of word in more than 200 language in which we have adjective adverb noun and verb grouped differently into a set of cognitive synonym where each word in the database is expressing it distinct concept The cognitive synonym which are called synset are presented in the database with lexical and semantic relation WordNET is publicly available for download and also we can test it network of related word and concept using this link Below are a few test image when accessed this through the browser.Where thesaurus is helping u in finding the synonym and antonym of the word the WordNET is helping u to do more than that WordNET interlinks the specific sense of the word wherein thesaurus link word by their meaning only In the WordNET the word are semantically disambiguated if they are in close proximity to each other Thesaurus provides a level to the word in the network if the word have similar meaning but in the case of WordNET we get level of word according to their semantic relation which is a better way of grouping the words.The below image is a basic structure of the WordNET The main concept of the relationship between the word in the WordNETs network is that the word are synonym like sad and unhappy benefit and profit These word show the same concept of using them in similar context by interchanging them These type of word are grouped into synset which are unordered set Where synset are linked together if they are having even small conceptual relation Every synset in the network ha it own brief definition and many of them are illustrated with the example of how to use them in a sentence That definition and example part make WordNET different from other In the below picture we can see the structure of any synset where we are having synonym of benefit in the array of synset with the definition and the example of usage of benefit word This synset is related to another synset word where the word benefit and profit have exactly the same meaning Here we can see the structure of the wordnet and also how the synset under the network are interlinked because of the conceptual relation between the word In linguistics a word with a broad meaning constitutes a category into which word with more specific meaning fall a superordinate For example the colour is a hypernym of red Where Hyponymy show the relationship between a hypernym and a specific instance of a hyponym A hyponym is a word or phrase whose semantic field is more specific than it hypernym The semantic field of a hypernym also known a a superordinate The reason for explaining these term here is because in WordNET the most frequent relationship between synset are based on these hyponym and hypernym relation These are very beneficial in linking word like paper piece of paper Saying more specifically with an example from the above picture like purple and violet in WordNET the category colour includes purple which in turn includes violet The root node of the hierarchy is the last point for every noun In violet is a kind of purple and purple is a kind of colour then violet is a kind colour this is the hyponymy relation between the word which is transitive The wordnet hold follows the meronymy relation which defines the whole relationship between the synset for example a bike ha two wheel handle and petrol tank These component of a bike are inherited from their subordinate if a bike ha two wheel then a sport bike ha wheel a well In linguistics we basically use this kind of relationship for adverb which basically represents the characteristic of the noun So the part are inherited into a downward direction because all the bike and type of bike have two wheel but not all kind of automobile consist of two wheel Adjective word under the WordNET arranged in the antonymy pair like wet and dry smile and cry Each of these pair of antonym is linked with set of semantic similar one The cry is linked to weep shed tear sob wail etc so that they all can be considered a the opposite of indirect antonym of a smile Most of the relation in the wordNET are in the same part of speech On the basis of part of speech relation we can divide WordNET into 4 type of 4 subnets one for each noun verb adjective and adverb There are also some cross-PoS pointer available in the network which include a morphosemantic link that hold the word with the same meaning and share a stem For example many pair like reader read in which the noun of the pair ha a semantic layer with respect to the verb have been specified Here in this article we had an overview of the WordNET along with an understanding of what are the basic structure of the wordnet and the synset We discussed how it work to make the relation between the word properly because the manageable representation of the data into the model can make a model more accurate and workable We saw what lexical relation that the database follows to hold the word with huge information and we have seen how we can implement this using python and nltk It can be done using TextBlob and R a well You can use WordNET and try it with these tool also And try to accurately implement it in the model for better accuracy\n",
      "--- 0.03787708282470703 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "sent = TextBlob(sentence2)\n",
    "start_time = time.time()\n",
    "print(\" \". join([w.lemmatize() for w in sent.words]))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# --- 0.03898167610168457 seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f71ebd",
   "metadata": {},
   "source": [
    "## Lemmatization using POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50faf66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Sentence without pos tag:\n",
      "the cat is sitting with the bats on the striped mat under many badly flying geese\n",
      "Before Mapping:\n",
      "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n",
      "After Mapping:\n",
      "[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), ('mat', 'n'), ('under', None), ('many', 'a'), ('badly', 'r'), ('flying', 'v'), ('geese', 'a')]\n",
      "Lemmatized Sentence with pos tag:\n",
      "the cat be sit with the bat on the striped mat under many badly fly geese\n"
     ]
    }
   ],
   "source": [
    "# Wordnet Lemmatizer (with POS tag) :\n",
    "# Wordnet results were not up to the mark. Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is \n",
    "# because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of \n",
    "# Speech) tags. \n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    " \n",
    "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
    "print('Lemmatized Sentence without pos tag:')\n",
    "print(lemmatizer.lemmatize(sentence))\n",
    "# tokenize the sentence and find the POS tag for each token AND before mapping\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "print('Before Mapping:')\n",
    "print(pos_tagged)\n",
    "\n",
    "# POS_TAGGER function mapping\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "print('After Mapping:')\n",
    "print(wordnet_tagged)\n",
    "\n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:       \n",
    "        # else use the tag to lemmatize the token\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    " \n",
    "print('Lemmatized Sentence with pos tag:')\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be90c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization before pos: the bat saw the cat with stripe hanging upside down by their foot\n",
      "Lemmatization after pos: the bat saw the cat with stripe hang upside down by their foot\n"
     ]
    }
   ],
   "source": [
    "# TextBlob\n",
    "\n",
    "from textblob import TextBlob\n",
    " \n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 2\n",
    "def pos_tagger(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "    words_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]   \n",
    "    lemma_list = [wd.lemmatize(tag) for wd, tag in words_tags]\n",
    "    return lemma_list\n",
    " \n",
    "# Lemmatize\n",
    "\n",
    "sentence = \"the bats saw the cats with stripes hanging upside down by their feet\"\n",
    "\n",
    "t_blob = TextBlob(sentence)\n",
    "lemmatized_sentence = \" \".join([w.lemmatize() for w in t_blob.words])\n",
    "print(f'Lemmatization before pos: {lemmatized_sentence}')\n",
    "\n",
    "lemma_list = pos_tagger(sentence)\n",
    "lemmatized_sentence = \" \".join(lemma_list)\n",
    "\n",
    "print(f'Lemmatization after pos: {lemmatized_sentence}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad653c",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1de13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] 179\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk, len(sw_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc8d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0049860477447509766 seconds ---\n",
      "Old length:  25\n",
      "New length:  533\n"
     ]
    }
   ],
   "source": [
    "text = \"When I first met her she was very quiet. She remained quiet during the entire two hour long journey from Stony Brook to New York.\"\n",
    "\n",
    "start_time = time.time()\n",
    "words = [word for word in sentence2.split() if word.lower() not in sw_nltk]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "new_text = \" \".join(words)\n",
    "# print(new_text)\n",
    "print(\"Old length: \", len(text.split()))\n",
    "print(\"New length: \", len(new_text.split()))\n",
    "\n",
    "# sw_nltk.extend(['first', 'second', 'third', 'me']) TO add words in stopwords list\n",
    "# sw_nltk.remove('not') TO remove\n",
    "\n",
    "# --- 0.004988431930541992 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1429f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "start_time = time.time()\n",
    "new_text = remove_stopwords(sentence2)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# print(new_text)\n",
    "# print(\"Old length: \", len(text.split()))\n",
    "# print(\"New length: \", len(new_text.split()))\n",
    "\n",
    "# --- 0.0009980201721191406 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ea0b23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/parsing/preprocessing.py\n",
    "STOPWORDS = ['all', 'six', 'just', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through',\n",
    "    'using', 'fifty', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere',\n",
    "    'much', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'yourselves', 'under',\n",
    "    'ours', 'two', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very',\n",
    "    'de', 'none', 'cannot', 'every', 'un', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'regarding',\n",
    "    'several', 'hereafter', 'did', 'always', 'who', 'didn', 'whither', 'this', 'someone', 'either', 'each', 'become',\n",
    "    'thereupon', 'sometime', 'side', 'towards', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'doing', 'km',\n",
    "    'eg', 'some', 'back', 'used', 'up', 'go', 'namely', 'computer', 'are', 'further', 'beyond', 'ourselves', 'yet',\n",
    "    'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its',\n",
    "    'everything', 'behind', 'does', 'various', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she',\n",
    "    'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere',\n",
    "    'although', 'found', 'alone', 're', 'along', 'quite', 'fifteen', 'by', 'both', 'about', 'last', 'would',\n",
    "    'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence',\n",
    "    'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others',\n",
    "    'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover',\n",
    "    'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due',\n",
    "    'been', 'next', 'anyone', 'eleven', 'cry', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves',\n",
    "    'hundred', 'really', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming',\n",
    "    'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'kg', 'herself', 'former', 'those', 'he', 'me', 'myself',\n",
    "    'made', 'twenty', 'these', 'was', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere',\n",
    "    'nine', 'can', 'whether', 'of', 'your', 'toward', 'my', 'say', 'something', 'and', 'whereafter', 'whenever',\n",
    "    'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'doesn', 'an', 'as', 'itself', 'at',\n",
    "    'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps',\n",
    "    'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which',\n",
    "    'becomes', 'you', 'if', 'nobody', 'unless', 'whereas', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon',\n",
    "    'eight', 'but', 'serious', 'nothing', 'such', 'why', 'off', 'a', 'don', 'whereby', 'third', 'i', 'whole', 'noone',\n",
    "    'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'with',\n",
    "    'make', 'once'\n",
    "]\n",
    "len(STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51d248d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0009968280792236328 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "start_time = time.time()\n",
    "words = [word for word in sentence2.split() if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "new_text = \" \".join(words)\n",
    "# print(new_text)\n",
    "# print(\"Old length: \", len(text.split()))\n",
    "# print(\"New length: \", len(new_text.split()))\n",
    "\n",
    "# --- 0.0009975433349609375 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20424f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    }
   ],
   "source": [
    "ENGLISH_STOP_WORDS = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost', \n",
    "                      'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', \n",
    "                      'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', \n",
    "                      'aren', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', \n",
    "                      'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', \n",
    "                      'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldn', \n",
    "                      'couldnt', 'cry', 'd', 'de', 'describe', 'detail', 'did', 'didn', 'do', 'does', 'doesn', 'doing', 'don', \n",
    "                      'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', \n",
    "                      'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', \n",
    "                      'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', \n",
    "                      'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'hadn', 'has', 'hasn', \n",
    "                      'hasnt', 'have', 'haven', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \n",
    "                      'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', \n",
    "                      'in', 'inc', 'indeed', 'interest', 'into', 'is', 'isn', 'it', 'its', 'itself', 'just', 'keep', 'last', \n",
    "                      'latter', 'latterly', 'least', 'less', 'll', 'ltd', 'm', 'ma', 'made', 'many', 'may', 'me', 'meanwhile', \n",
    "                      'might', 'mightn', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', \n",
    "                      'mustn', 'my', 'myself', 'name', 'namely', 'needn', 'neither', 'never', 'nevertheless', 'next', 'nine', \n",
    "                      'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'o', 'of', 'off', 'often', \n",
    "                      'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', \n",
    "                      'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', \n",
    "                      'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'shan', 'she', 'should', 'shouldn', 'show', \n",
    "                      'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', \n",
    "                      'sometimes', 'somewhere', 'still', 'such', 'system', 't', 'take', 'ten', 'than', 'that', 'the', 'their', \n",
    "                      'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', \n",
    "                      'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', \n",
    "                      'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', \n",
    "                      'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 've', 'very', 'via', 'was', 'wasn', 'we', \n",
    "                      'well', 'were', 'weren', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', \n",
    "                      'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \n",
    "                      'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', 'would', 'wouldn', \n",
    "                      'y', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "print(len(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fd23bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.013963460922241211 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py\n",
    "\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "start_time = time.time()\n",
    "tokens_without_sw = [word for word in word_tokenize(sentence2) if not word in all_stopwords]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print(\"Old length: \", len(sentence2))\n",
    "# print(\"New length: \", len(tokens_without_sw))\n",
    "# --- 0.01595592498779297 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e0aa5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'ca', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'just', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'until', 'up', 'unless', 'upon', 'us', 'used', 'using', 'various', 'very', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "spacy_sw = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'ca', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'just', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'until', 'up', 'unless', 'upon', 'us', 'used', 'using', 'various', 'very', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "len(spacy_sw)\n",
    "print(spacy_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4b1fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sw = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "gensim_sw = STOPWORDS\n",
    "sklearn_sw = ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e487e19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for nltk: 0.004987 sec\n",
      "Execution time for gensim: 0.00102 sec\n",
      "Execution time for sklearn: 0.000997 sec\n",
      "Execution time for spacy: 0.015956 sec\n",
      "nltk:\n",
      "i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\n",
      "'\n",
      "\n",
      "\n",
      "spacy_sw(306) but not in nltk(179)\n",
      "\n",
      "across, afterwards, almost, alone, along, already, also, although, always, among, amongst, amount, another, anyhow, anyone, anything, anyway, anywhere, around, back, became, become, becomes, becoming, beforehand, behind, beside, besides, beyond, bottom, call, cannot, ca, could, done, due, eight, either, eleven, else, elsewhere, empty, enough, even, ever, every, everyone, everything, everywhere, except, fifteen, fifty, first, five, former, formerly, forty, four, front, full, get, give, go, hence, hereafter, hereby, herein, hereupon, however, hundred, indeed, keep, last, latter, latterly, least, less, made, make, many, may, meanwhile, might, mine, moreover, mostly, move, much, must, name, namely, neither, never, nevertheless, next, nine, nobody, none, noone, nothing, nowhere, often, one, onto, others, otherwise, part, per, perhaps, please, put, quite, rather, really, regarding, say, see, seem, seemed, seeming, seems, serious, several, show, side, since, six, sixty, somehow, someone, something, sometime, sometimes, somewhere, still, take, ten, thence, thereafter, thereby, therefore, therein, thereupon, third, though, three, throughout, thru, thus, together, top, toward, towards, twelve, twenty, two, unless, upon, us, used, using, various, via, well, whatever, whence, whenever, whereafter, whereas, whereby, wherein, whereupon, wherever, whether, whither, whoever, whole, whose, within, without, would, yet, \n",
      "182\n",
      "\n",
      "gensim(337) but not in nltk(179)\n",
      "\n",
      "six, less, indeed, move, anyway, four, using, fifty, mill, find, one, whose, system, somewhere, much, thick, show, enough, must, seeming, two, might, thereafter, latterly, around, get, de, none, cannot, every, un, front, thus, name, regarding, several, hereafter, always, whither, someone, either, become, thereupon, sometime, side, towards, therein, twelve, often, ten, km, eg, back, used, go, namely, computer, beyond, yet, even, still, bottom, mine, since, please, forty, per, everything, behind, various, neither, seemed, ever, across, somehow, full, never, sixty, however, otherwise, whereupon, nowhere, although, found, alone, along, quite, fifteen, last, would, anything, via, many, could, thence, put, keep, etc, amount, became, ltd, hence, onto, con, among, already, co, afterwards, formerly, within, seems, others, whatever, except, everyone, done, least, another, whoever, moreover, couldnt, throughout, anyhow, three, together, top, due, next, anyone, eleven, cry, call, therefore, interest, thru, hundred, really, sincere, empty, elsewhere, mostly, fire, becoming, hereby, amongst, else, part, everywhere, kg, former, made, twenty, bill, cant, us, besides, nevertheless, anywhere, nine, whether, toward, say, something, whereafter, whenever, give, almost, wherever, describe, beforehand, herein, seem, whence, ie, fill, hasnt, inc, thereby, thin, perhaps, latter, meanwhile, detail, wherein, beside, also, take, becomes, nobody, unless, whereas, see, though, may, upon, hereupon, eight, serious, nothing, whereby, third, whole, noone, sometimes, well, amoungst, rather, without, five, first, make, \n",
      "211\n",
      "\n",
      "\n",
      "sklearn(352) but not in nltk(179)\n",
      "\n",
      "across, afterwards, almost, alone, along, already, also, although, always, among, amongst, amoungst, amount, another, anyhow, anyone, anything, anyway, anywhere, around, back, became, become, becomes, becoming, beforehand, behind, beside, besides, beyond, bill, bottom, call, cannot, cant, co, con, could, couldnt, cry, de, describe, detail, done, due, eg, eight, either, eleven, else, elsewhere, empty, enough, etc, even, ever, every, everyone, everything, everywhere, except, fifteen, fify, fill, find, fire, first, five, former, formerly, forty, found, four, front, full, get, give, go, hasnt, hence, hereafter, hereby, herein, hereupon, however, hundred, ie, inc, indeed, interest, keep, last, latter, latterly, least, less, ltd, made, many, may, meanwhile, might, mill, mine, moreover, mostly, move, much, must, name, namely, neither, never, nevertheless, next, nine, nobody, none, noone, nothing, nowhere, often, one, onto, others, otherwise, part, per, perhaps, please, put, rather, see, seem, seemed, seeming, seems, serious, several, show, side, since, sincere, six, sixty, somehow, someone, something, sometime, sometimes, somewhere, still, system, take, ten, thence, thereafter, thereby, therefore, therein, thereupon, thick, thin, third, though, three, throughout, thru, thus, together, top, toward, towards, twelve, twenty, two, un, upon, us, via, well, whatever, whence, whenever, whereafter, whereas, whereby, wherein, whereupon, wherever, whether, whither, whoever, whole, whose, within, without, would, yet, \n",
      "199\n",
      "\n",
      "\n",
      "sklearn(352) but not in gensim(337)\n",
      "\n",
      "ain, aren, couldn, d, fify, hadn, hasn, haven, having, isn, ll, m, ma, mightn, mustn, needn, o, s, shan, shouldn, t, theirs, ve, wasn, weren, won, wouldn, y, \n",
      "28\n"
     ]
    }
   ],
   "source": [
    "def findMissing(a, b):\n",
    "    counter=0\n",
    "    s = dict()\n",
    "    for i in range(len(b)):\n",
    "        s[b[i]] = 1\n",
    " \n",
    "    # Print all elements of first array\n",
    "    # that are not present in hash table\n",
    "    for i in range(len(a)):\n",
    "        if a[i] not in s.keys():\n",
    "            counter+=1\n",
    "            print(a[i], end = \", \")\n",
    "            \n",
    "    print(f'\\n{counter}')\n",
    "    \n",
    "print(f'Execution time for nltk: 0.004987 sec')\n",
    "print(f'Execution time for gensim: 0.00102 sec')\n",
    "print(f'Execution time for sklearn: 0.000997 sec')\n",
    "print(f'Execution time for spacy: 0.015956 sec')\n",
    "print(f\"nltk:\\n{', '.join(nltk_sw)}\\n'\")\n",
    "print(f'\\n\\nspacy_sw({len(spacy_sw)}) but not in nltk({len(nltk_sw)})\\n')\n",
    "findMissing(spacy_sw, nltk_sw)\n",
    "print(f'\\ngensim({len(gensim_sw)}) but not in nltk({len(nltk_sw)})\\n')\n",
    "findMissing(gensim_sw, nltk_sw)\n",
    "print(f'\\n\\nsklearn({len(sklearn_sw)}) but not in nltk({len(nltk_sw)})\\n')\n",
    "findMissing(sklearn_sw, nltk_sw)\n",
    "print(f'\\n\\nsklearn({len(sklearn_sw)}) but not in gensim({len(gensim_sw)})\\n')\n",
    "findMissing(sklearn_sw, gensim_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c947a20",
   "metadata": {},
   "source": [
    "## Bags of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e65d2860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   all  anything  at  for  good  is  it  job  miss  not  this  will\n",
      "0    0         1   0    1     1   1   1    1     1    1     1     1\n",
      "1    1         0   1    0     1   1   0    0     0    1     1     0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "# A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of \n",
    "# word counts and disregard the grammatical details and the word order. It only considers the occurence\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "sentence_1=\"This is a good job.I will not miss it for anything\"\n",
    "sentence_2=\"This is not good at all\"\n",
    " \n",
    " \n",
    " \n",
    "CountVec = CountVectorizer(ngram_range=(1,1))  #, stop_words='english')\n",
    "# (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams\n",
    "\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "print(cv_dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a93210a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "('You',)\n",
      "('will',)\n",
      "('face',)\n",
      "('many',)\n",
      "('defeats',)\n",
      "('in',)\n",
      "('life.',)\n",
      "Bigram:\n",
      "('You', 'will')\n",
      "('will', 'face')\n",
      "('face', 'many')\n",
      "('many', 'defeats')\n",
      "('defeats', 'in')\n",
      "('in', 'life.')\n",
      "Trigram:\n",
      "('You', 'will', 'face')\n",
      "('will', 'face', 'many')\n",
      "('face', 'many', 'defeats')\n",
      "('many', 'defeats', 'in')\n",
      "('defeats', 'in', 'life.')\n"
     ]
    }
   ],
   "source": [
    "# Unigram\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = 'You will face many defeats in life.'\n",
    "unigrams = ngrams(sentence.split(), 1)\n",
    "print('Unigram:')\n",
    "for item in unigrams:\n",
    "    print(item)\n",
    "    \n",
    "unigrams = ngrams(sentence.split(), 2)\n",
    "print('Bigram:')\n",
    "for item in unigrams:\n",
    "    print(item)\n",
    "          \n",
    "unigrams = ngrams(sentence.split(), 3)\n",
    "print('Trigram:')\n",
    "for item in unigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae2fc801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   at all  for anything  good at  good job  is good  is not  it for  job will  \\\n",
      "0       0             1        0         1        1       0       1         1   \n",
      "1       1             0        1         0        0       1       0         0   \n",
      "\n",
      "   miss it  not good  not miss  this is  will not  \n",
      "0        1         0         1        1         1  \n",
      "1        0         1         0        1         0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(2,2))  #, stop_words='english')\n",
    "# (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams\n",
    "\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "print(cv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5714b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   good at all  good job will  is good job  is not good  it for anything  \\\n",
      "0            0              1            1            0                1   \n",
      "1            1              0            0            1                0   \n",
      "\n",
      "   job will not  miss it for  not good at  not miss it  this is good  \\\n",
      "0             1            1            0            1             1   \n",
      "1             0            0            1            0             0   \n",
      "\n",
      "   this is not  will not miss  \n",
      "0            0              1  \n",
      "1            1              0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Trigram\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(3,3))  #, stop_words='english')\n",
    "# (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams\n",
    "\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "print(cv_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b759f",
   "metadata": {},
   "source": [
    "## TfiDf (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12bf3a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:  ['cars', 'cheaper', 'diesel', 'petrol']\n",
      "Sparse Matrix \n",
      " (2, 4) \n",
      " [[0.85135433 0.30287281 0.30287281 0.30287281]\n",
      " [0.         0.57735027 0.57735027 0.57735027]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency (tf)\n",
    "# tf is the number of times a term appears in a particular document.\n",
    "# tf(t) = (No. of times term ‘t’ occurs in a document) / (No. Of terms in a document)\n",
    "\n",
    "# Inverse Document Frequency (idf)\n",
    "# idf is a measure of how common or rare a term is across the entire corpus of documents.\n",
    "# If the word is common and appears in many documents, the idf value will approach 0 or else approach 1 if it’s rare.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "d1=\"petrol cars are cheaper than diesel cars\"\n",
    "d2=\"diesel is cheaper than petrol\"\n",
    "\n",
    "doc_corpus=[d1,d2]\n",
    "\n",
    "vec=TfidfVectorizer(stop_words='english')\n",
    "matrix=vec.fit_transform(doc_corpus)\n",
    "print(\"Feature Names: \",vec.get_feature_names())\n",
    "\n",
    "print(\"Sparse Matrix \\n\", matrix.shape,\"\\n\",matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7789144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(2, dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1], [2, 0, 1]]\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=3, radius=0.4)\n",
    "neigh.fit(samples)\n",
    "print(neigh.kneighbors([[0, 0, 1.3]], 3, return_distance=False))\n",
    "\n",
    "nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)\n",
    "np.asarray(nbrs[0][0])\n",
    "# array(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf766d7",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a27aa8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1117, 5248)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These models are shallow two-layer neural networks having one input layer, one hidden layer, and one output layer.\n",
    "# CBOW (Continuous Bag of Words): CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. \n",
    "# Skip Gram : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words.\n",
    "# https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "\n",
    "model1 = gensim.models.Word2Vec(sentence2, min_count = 1, vector_size = 100, window = 5)\n",
    "model1.train(sentence2.split(), total_examples=1, epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93e7a0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x18a446b5708>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = model1.wv\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3e25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddbd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e846159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f08676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d707377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21597951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10.,  2.],\n",
       "       [ 1.,  2.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, algorithm='full').fit(X)\n",
    "kmeans.labels_\n",
    "\n",
    "kmeans.predict([[0, 0], [12, 3]])\n",
    "\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f056c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240beef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
